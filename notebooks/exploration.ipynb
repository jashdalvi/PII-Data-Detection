{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('../data/train.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-EMAIL',\n",
       " 'B-ID_NUM',\n",
       " 'B-NAME_STUDENT',\n",
       " 'B-PHONE_NUM',\n",
       " 'B-STREET_ADDRESS',\n",
       " 'B-URL_PERSONAL',\n",
       " 'B-USERNAME',\n",
       " 'I-ID_NUM',\n",
       " 'I-NAME_STUDENT',\n",
       " 'I-PHONE_NUM',\n",
       " 'I-STREET_ADDRESS',\n",
       " 'I-URL_PERSONAL',\n",
       " 'O']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Design', 'Thinking', 'for', 'innovation', 'reflexion', '-', 'Avril', '2021', '-', 'Nathalie']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT']\n",
      "[True, True, True, True, False, False, True, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "x = data[0]\n",
    "\n",
    "print(x[\"tokens\"][:10])\n",
    "print(x[\"labels\"][:10])\n",
    "print(x[\"trailing_whitespace\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-EMAIL',\n",
       " 1: 'B-ID_NUM',\n",
       " 2: 'B-NAME_STUDENT',\n",
       " 3: 'B-PHONE_NUM',\n",
       " 4: 'B-STREET_ADDRESS',\n",
       " 5: 'B-URL_PERSONAL',\n",
       " 6: 'B-USERNAME',\n",
       " 7: 'I-ID_NUM',\n",
       " 8: 'I-NAME_STUDENT',\n",
       " 9: 'I-PHONE_NUM',\n",
       " 10: 'I-STREET_ADDRESS',\n",
       " 11: 'I-URL_PERSONAL',\n",
       " 12: 'O'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "\n",
    "# these are at the character level\n",
    "labels = []\n",
    "\n",
    "for t, l, ws in zip(example[\"tokens\"], example[\"labels\"], example[\"trailing_whitespace\"]):\n",
    "\n",
    "    text.append(t)\n",
    "    labels.extend([l]*len(t))\n",
    "\n",
    "    # if there is trailing whitespace\n",
    "    if ws:\n",
    "        text.append(\" \")\n",
    "        labels.append(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1320"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3709"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jashdalvi/miniforge3/envs/ml/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 6),\n",
       " (6, 15),\n",
       " (15, 19),\n",
       " (19, 30),\n",
       " (30, 37),\n",
       " (37, 40),\n",
       " (40, 41),\n",
       " (41, 43),\n",
       " (43, 46)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[\"offset_mapping\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3709"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6\n",
      "[12, 12]\n",
      "7 15\n",
      "[12, 12, 12]\n",
      "16 19\n",
      "[12, 12, 12, 12]\n",
      "20 30\n",
      "[12, 12, 12, 12, 12]\n"
     ]
    }
   ],
   "source": [
    "token_labels = []\n",
    "for start_idx, end_idx in tokenized.offset_mapping[:5]:\n",
    "\n",
    "    # CLS token\n",
    "    if start_idx == 0 and end_idx == 0: \n",
    "        token_labels.append(label2id[\"O\"])\n",
    "        continue\n",
    "    \n",
    "    # case when token starts with whitespace\n",
    "    if text[start_idx].isspace():\n",
    "        start_idx += 1\n",
    "    \n",
    "    while start_idx >= len(labels):\n",
    "        start_idx -= 1\n",
    "        \n",
    "    token_labels.append(label2id[labels[start_idx]])\n",
    "    print(start_idx, end_idx)\n",
    "    print(token_labels)\n",
    "    \n",
    "length = len(tokenized.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('../data/train.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [doc[\"document\"] for doc in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id_to_text_mapping = {doc[\"full_text\"]: doc[\"document\"] for doc in train_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6807/6807 [00:00<00:00, 64711.60it/s]\n"
     ]
    }
   ],
   "source": [
    "data_with_id = []\n",
    "for doc in tqdm(data, total=len(data)):\n",
    "    doc_id = doc_id_to_text_mapping[doc[\"full_text\"]]\n",
    "    doc[\"doc_id\"] = doc_id\n",
    "    data_with_id.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 has 1698 documents : 24.94%\n",
      "Fold 1 has 1714 documents : 25.18%\n",
      "Fold 2 has 1689 documents : 24.81%\n",
      "Fold 3 has 1706 documents : 25.06%\n"
     ]
    }
   ],
   "source": [
    "for fold in [0,1,2,3]:\n",
    "    fold_data = [doc for doc in train_data if doc[\"document\"] % 4 == fold]\n",
    "    print(f\"Fold {fold} has {len(fold_data)} documents : {len(fold_data)/len(train_data) *  100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6807"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6807"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

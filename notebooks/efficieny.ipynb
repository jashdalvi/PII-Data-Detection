{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def get_merged_preds_vectorized(predictions, token_idxs_mapping):\n",
    "    # Ensure predictions is a 2D array for compatibility with DataFrame\n",
    "    predictions = np.atleast_2d(predictions)\n",
    "    \n",
    "    # Create a DataFrame from token_idxs_mapping and predictions\n",
    "    df = pd.DataFrame({\n",
    "        'token_idx': token_idxs_mapping,\n",
    "        'predictions': list(predictions)\n",
    "    })\n",
    "    \n",
    "    # Exclude -1 from averaging, if necessary\n",
    "    df = df[df['token_idx'] != -1]\n",
    "    \n",
    "    # Group by token_idx and average predictions\n",
    "    averaged_df = df.groupby('token_idx')['predictions'].apply(lambda x: np.mean(np.vstack(x), axis=0)).reset_index()\n",
    "    \n",
    "    # Map averaged predictions back to the original order\n",
    "    averaged_predictions = np.array(df['token_idx'].map(averaged_df.set_index('token_idx')['predictions']).tolist())\n",
    "    \n",
    "    # Handle -1 indices if necessary, assuming original predictions for -1 indices are kept\n",
    "    minus_one_indices = np.where(token_idxs_mapping == -1)[0]\n",
    "    if minus_one_indices.size > 0:\n",
    "        for idx in minus_one_indices:\n",
    "            averaged_predictions[idx] = predictions[idx]\n",
    "    \n",
    "    return np.array(averaged_predictions)\n",
    "\n",
    "\n",
    "def get_merged_preds(predictions, token_idxs_mapping):\n",
    "    if token_idxs_mapping is not None:\n",
    "        # Initialize averaged_predictions with the same shape as predictions\n",
    "        averaged_predictions = np.array(predictions)\n",
    "\n",
    "        unique_token_idxs = set(token_idxs_mapping).difference(set([-1]))\n",
    "        \n",
    "        # Iterate over each unique token index to average predictions\n",
    "        for token_idx in unique_token_idxs:\n",
    "            # Find the indices in token_idxs_mapping that match the current token_idx\n",
    "            indices = np.where(np.array(token_idxs_mapping) == token_idx)[0]\n",
    "            \n",
    "            # Average the predictions for these indices and assign to the correct positions\n",
    "            averaged_predictions[indices] = np.mean(predictions[indices], axis=0)\n",
    "        \n",
    "        # Use the averaged predictions for further processing\n",
    "        predictions = averaged_predictions\n",
    "    \n",
    "    return predictions, predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer, label2id, max_length):\n",
    "    \"\"\"tokenize the examples\"\"\"\n",
    "    text = []\n",
    "    labels = []\n",
    "    token_map = [] # Character index to spacy token mapping\n",
    "\n",
    "    token_map_idx = 0\n",
    "    for t, l, ws in zip(example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]):\n",
    "        text.append(t)\n",
    "        labels.extend([l]*len(t))\n",
    "        token_map.extend([token_map_idx] * len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            labels.append(\"O\")\n",
    "            token_map.append(-1)\n",
    "\n",
    "        token_map_idx += 1\n",
    "\n",
    "\n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation = True, max_length=max_length, return_overflowing_tokens=True, stride = 256)\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    text = \"\".join(text)\n",
    "    token_labels = []\n",
    "    token_idxs_mapping = [] # Represents the mapping of deberta token idx to spacy token idx. We can potentially merge the predictions of these tokens\n",
    "    num_sequences = len(tokenized[\"input_ids\"])\n",
    "    for sequence_idx in range(num_sequences):\n",
    "        offset_mapping_sequence = tokenized[\"offset_mapping\"][sequence_idx]\n",
    "        token_labels_sequence = []\n",
    "        token_idxs_mapping_sequence = []\n",
    "        for start_idx, end_idx in offset_mapping_sequence:\n",
    "            \n",
    "            # CLS token\n",
    "            if start_idx == 0 and end_idx == 0: \n",
    "                token_idxs_mapping_sequence.append(-1)\n",
    "                token_labels_sequence.append(label2id[\"O\"])\n",
    "                continue\n",
    "            \n",
    "            # case when token starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "            \n",
    "            while start_idx >= len(labels):\n",
    "                start_idx -= 1\n",
    "                \n",
    "            token_labels_sequence.append(label2id[labels[start_idx]])\n",
    "            token_idxs_mapping_sequence.append(token_map[start_idx])\n",
    "        \n",
    "        token_labels.append(token_labels_sequence)\n",
    "        token_idxs_mapping.append(token_idxs_mapping_sequence)\n",
    "    \n",
    "    token_map = [token_map for _ in range(num_sequences)]\n",
    "    document = [example[\"document\"] for _ in range(num_sequences)]\n",
    "    fold = [example[\"fold\"] for _ in range(num_sequences)]\n",
    "    tokens = [example[\"tokens\"] for _ in range(num_sequences)]\n",
    "        \n",
    "    return {\n",
    "        **tokenized,\n",
    "        \"labels\": token_labels,\n",
    "        \"token_map\": token_map,\n",
    "        \"document\": document,\n",
    "        \"fold\": fold,\n",
    "        \"tokens\": tokens,\n",
    "        \"token_idxs_mapping\": token_idxs_mapping\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add labels as global variable\n",
    "LABELS = ['B-EMAIL',\n",
    "        'B-ID_NUM',\n",
    "        'B-NAME_STUDENT',\n",
    "        'B-PHONE_NUM',\n",
    "        'B-STREET_ADDRESS',\n",
    "        'B-URL_PERSONAL',\n",
    "        'B-USERNAME',\n",
    "        'I-ID_NUM',\n",
    "        'I-NAME_STUDENT',\n",
    "        'I-PHONE_NUM',\n",
    "        'I-STREET_ADDRESS',\n",
    "        'I-URL_PERSONAL',\n",
    "        'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jashdalvi/miniforge3/envs/ml/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa69d1146e0d40a6af7e623b9fa016b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jashdalvi/miniforge3/envs/ml/lib/python3.9/site-packages/datasets/table.py:1398: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/Users/jashdalvi/miniforge3/envs/ml/lib/python3.9/site-packages/datasets/table.py:1424: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/train.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "ds = Dataset.from_dict({\n",
    "    \"full_text\": [x[\"full_text\"] for x in data],\n",
    "    \"document\": [x[\"document\"] for x in data],\n",
    "    \"tokens\": [x[\"tokens\"] for x in data],\n",
    "    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "    \"provided_labels\": [x[\"labels\"] for x in data],\n",
    "    \"fold\": [x[\"document\"] % 4 for x in data]\n",
    "})\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(LABELS)}\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "ds = ds.map(\n",
    "    tokenize, \n",
    "    fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": 1024},\n",
    "    num_proc = 4\n",
    ").remove_columns([\"full_text\", \"trailing_whitespace\", \"provided_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_flatten_ds(ds):\n",
    "    features = list(ds.features.keys())\n",
    "    dataset_dict = {feature: [] for feature in features}\n",
    "\n",
    "    for example in tqdm(ds, total=len(ds)):\n",
    "        #Also make sure everything is a list\n",
    "        for feature in features:\n",
    "            assert isinstance(example[feature], list), f\"Feature {feature} is not a list\"\n",
    "        for feature in features:\n",
    "            dataset_dict[feature].extend(example[feature])\n",
    "\n",
    "    return Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6807/6807 [00:27<00:00, 243.17it/s]\n"
     ]
    }
   ],
   "source": [
    "ds = build_flatten_ds(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79680684b2c949fa86448fd0660e9847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/7605 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jashdalvi/miniforge3/envs/ml/lib/python3.9/site-packages/datasets/table.py:1398: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/Users/jashdalvi/miniforge3/envs/ml/lib/python3.9/site-packages/datasets/table.py:1424: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "valid_ds = ds.filter(lambda x: x[\"fold\"] == 0, num_proc = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'tokens', 'fold', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'labels', 'token_map', 'token_idxs_mapping'],\n",
       "    num_rows: 1899\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [np.random.rand(len(offsets), 13) for offsets in valid_ds[\"offset_mapping\"]]\n",
    "token_idxs_mapping = valid_ds[\"token_idxs_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1899, 1899)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds), len(token_idxs_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-vectorized method took 34.8940 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "for i, pred in enumerate(preds):\n",
    "    averaged_predictions = get_merged_preds(pred, token_idxs_mapping[i])\n",
    "toc = time.perf_counter()\n",
    "print(f\"Non-vectorized method took {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel method took 8.6647 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "new_preds = Parallel(n_jobs=8)(delayed(get_merged_preds)(preds[i], token_idxs_mapping[i]) for i in range(len(preds)))\n",
    "toc = time.perf_counter()\n",
    "print(f\"Parallel method took {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1899, 2)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_preds), len(new_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jashdalvi/miniforge3/envs/ml/lib/python3.9/site-packages/numpy/core/shape_base.py:121: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m----> 2\u001b[0m averaged_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mget_merged_preds_vectorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_idxs_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectorized method took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoc\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtic\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[49], line 14\u001b[0m, in \u001b[0;36mget_merged_preds_vectorized\u001b[0;34m(predictions, token_idxs_mapping)\u001b[0m\n\u001b[1;32m     11\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_2d(predictions)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create a DataFrame from token_idxs_mapping and predictions\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoken_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_idxs_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Exclude -1 from averaging, if necessary\u001b[39;00m\n\u001b[1;32m     20\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_idx\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/pandas/core/frame.py:733\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    727\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    728\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    729\u001b[0m     )\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "averaged_predictions = get_merged_preds_vectorized(preds, token_idxs_mapping)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Vectorized method took {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-vectorized method took 34.9694 seconds\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
